#!/bin/bash

# EMR on EKS Job ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (Lake Formation FGAC + S3 Tables)
# Lake Formation FGAC 4ê°œ ì—­í• ë³„ ë¶„ì„ Job ì‹¤í–‰
# S3 Tables (Apache Iceberg) ì¹´íƒˆë¡œê·¸ ì‚¬ìš©
# EMR on EKS Blueprintì˜ Job Templateê³¼ Pod Template í™œìš©

set -e

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
if [ ! -f ".env" ]; then
    echo "âŒ .env íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € 04-setup-emr-on-eks.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

source .env

# Lake Formation FGAC ì„¤ì • í™•ì¸
if [ -z "$LF_VIRTUAL_CLUSTER_ID" ]; then
    echo "âŒ Lake Formation FGACê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ë¨¼ì € ./scripts/04-1-setup-lake-formation-fgac.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

echo "=== EMR on EKS Job ì‹¤í–‰ ì‹œì‘ (Lake Formation FGAC í™œì„±í™”) ==="
echo "LF Virtual Cluster ID: $LF_VIRTUAL_CLUSTER_ID"
echo "Security Configuration: $SECURITY_CONFIG_ID"
echo "Session Tag Value: $LF_SESSION_TAG_VALUE"
echo "User Namespace: $USER_NAMESPACE"
echo "Scripts Bucket: s3://$SCRIPTS_BUCKET"
echo "Job Templates Directory: ./job-templates/"
echo "Pod Templates Directory: ./pod-templates/"
echo ""

# Job ì„¤ì • (Lake Formation FGAC ì—­í•  ë§¤í•‘)
JOB_CONFIGS=(
    "data-steward:emr-data-steward-sa:$LF_DATA_STEWARD_ROLE:ì „ì²´ ë°ì´í„° ë¶„ì„ (100,000ê±´)"
    #"gangnam-analytics:emr-gangnam-analytics-sa:$LF_GANGNAM_ANALYTICS_ROLE:ê°•ë‚¨êµ¬ ë°ì´í„° ë¶„ì„ (~3,000ê±´)"
    #"operation:emr-operation-sa:$LF_OPERATION_ROLE:ìš´ì˜ ë°ì´í„° ë¶„ì„ (ê°œì¸ì •ë³´ ì œì™¸)"
    #"marketing-partner:emr-marketing-partner-sa:$LF_MARKETING_PARTNER_ROLE:ë§ˆì¼€íŒ… íƒ€ê²Ÿ ë¶„ì„ (ê°•ë‚¨êµ¬ 20-30ëŒ€)"
)

# ê²°ê³¼ ì €ì¥ìš© S3 ë²„í‚·
RESULTS_BUCKET="seoul-bike-analytics-results-${ACCOUNT_ID}"
aws s3 mb s3://$RESULTS_BUCKET --region $REGION 2>/dev/null || echo "ê²°ê³¼ ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."

# í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p job-templates pod-templates

echo ""
echo "â„¹ï¸  Lake Formation FGAC + S3 Tablesê°€ í™œì„±í™”ëœ Virtual Clusterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
echo "â„¹ï¸  S3 Tables (Apache Iceberg) ì¹´íƒˆë¡œê·¸: s3tablescatalog"
echo "â„¹ï¸  EMR on EKS Blueprint Job Templateê³¼ Pod Templateì„ í™œìš©í•©ë‹ˆë‹¤."
echo "â„¹ï¸  Security Configuration: $SECURITY_CONFIG_ID"
echo "â„¹ï¸  Session Tag: LakeFormationAuthorizedCaller=$LF_SESSION_TAG_VALUE"

# Job Template ìƒì„± í•¨ìˆ˜ (Lake Formation FGAC + S3 Tables ìµœì í™”)
create_job_template() {
    local job_name=$1
    local service_account=$2
    local role_name=$3
    local timestamp=$(date +%Y%m%d-%H%M%S)
    
    echo "ğŸ“ $job_name Job Template ìƒì„± ì¤‘..."
    
    # Job Template íŒŒì¼ ìƒì„±
    local job_template_file="job-templates/${job_name}-job-template.json"
    
    cat > "$job_template_file" << EOF
{
  "name": "seoul-bike-${job_name}-${timestamp}",
  "virtualClusterId": "$LF_VIRTUAL_CLUSTER_ID",
  "executionRoleArn": "arn:aws:iam::${ACCOUNT_ID}:role/${role_name}",
  "releaseLabel": "emr-7.7.0-latest",
  "jobDriver": {
    "sparkSubmitJobDriver": {
      "entryPoint": "s3://${SCRIPTS_BUCKET}/spark-jobs/${job_name}-analysis.py",
      "sparkSubmitParameters": "--conf spark.executor.instances=2 --conf spark.executor.memory=1g --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.driver.memory=1g"
    }
  },
  "configurationOverrides": {
    "applicationConfiguration": [
      {
        "classification": "spark-defaults",
        "properties": {
          "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
          "spark.sql.catalog.s3tablesbucket": "org.apache.iceberg.spark.SparkCatalog",
          "spark.sql.catalog.s3tablesbucket.catalog-impl": "software.amazon.s3tables.iceberg.S3TablesCatalog",
          "spark.sql.catalog.s3tablesbucket.warehouse": "arn:aws:s3tables:$REGION:$ACCOUNT_ID:bucket/${TABLE_BUCKET_NAME}",
          "spark.sql.catalog.s3tablesbucket.client.region": "$REGION",
          "spark.sql.defaultCatalog": "s3tablesbucket",
          "spark.kubernetes.driver.podTemplateFile": "s3://seoul-bike-analytics-scripts-${ACCOUNT_ID}/pod-templates/driver-pod-template.yaml",
          "spark.kubernetes.executor.podTemplateFile": "s3://seoul-bike-analytics-scripts-${ACCOUNT_ID}/pod-templates/executor-pod-template.yaml",
          "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider",
          "spark.hadoop.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider",
          "spark.hadoop.fs.s3a.endpoint.region": "$REGION",
          "spark.hadoop.fs.s3.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
          "spark.hadoop.aws.region": "$REGION",
          "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
          "spark.executor.instances": "2",
          "spark.executor.memory": "1g",
          "spark.executor.cores": "1",
          "spark.driver.memory": "1g",
          "spark.driver.cores": "1"
        }
      }
    ],
    "monitoringConfiguration": {
      "persistentAppUI": "ENABLED",
      "cloudWatchMonitoringConfiguration": {
        "logGroupName": "/aws/emr-containers/jobs",
        "logStreamNamePrefix": "${job_name}"
      },
      "s3MonitoringConfiguration": {
        "logUri": "s3://${RESULTS_BUCKET}/logs/"
      }
    }
  },
  "tags": {
    "LakeFormationAuthorizedCaller": "$LF_SESSION_TAG_VALUE",
    "JobType": "LakeFormationFGAC",
    "Role": "${role_name}",
    "Namespace": "$USER_NAMESPACE",
    "CatalogType": "S3Tables"
  }
}
EOF
    
    echo "   âœ… Job Template ìƒì„± ì™„ë£Œ: $job_template_file"
    return 0
}

# Pod Template ìƒì„± í•¨ìˆ˜
create_pod_template() {
    local job_name=$1
    local service_account=$2
    
    echo "ğŸ“ $job_name Pod Template ìƒì„± ì¤‘..."
    
    # Pod Template íŒŒì¼ ìƒì„±
    local pod_template_file="pod-templates/${job_name}-pod-template.yaml"
    
    cat > "$pod_template_file" << EOF
apiVersion: v1
kind: Pod
metadata:
  name: spark-${job_name}-template
  namespace: $USER_NAMESPACE
  labels:
    app: spark-${job_name}
    version: "1.0"
    component: spark-executor
    spark-role: executor
    job-type: lake-formation-fgac
    LakeFormationAuthorizedCaller: "$LF_SESSION_TAG_VALUE"
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/metrics/executors/prometheus/"
    prometheus.io/port: "4040"
    LakeFormationAuthorizedCaller: "$LF_SESSION_TAG_VALUE"
spec:
  serviceAccountName: ${service_account}
  restartPolicy: Never
  nodeSelector:
    karpenter.sh/nodepool: spark-compute-optimized
    node.kubernetes.io/instance-type: "c5.large"
  tolerations:
    - key: spark-compute-optimized
      operator: Equal
      value: "true"
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: karpenter.sh/nodepool
            operator: In
            values:
            - spark-compute-optimized
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: spark-role
              operator: In
              values:
              - executor
          topologyKey: kubernetes.io/hostname
  containers:
  - name: spark-kubernetes-executor
    image: public.ecr.aws/emr-on-eks/spark/emr-7.8.0:latest
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
        ephemeral-storage: "2Gi"
      limits:
        memory: "1Gi"
        cpu: "1"
        ephemeral-storage: "4Gi"
    env:
    - name: SPARK_CONF_DIR
      value: /opt/spark/conf
    - name: AWS_REGION
      value: ${REGION}
    - name: AWS_DEFAULT_REGION
      value: ${REGION}
    - name: SPARK_LOCAL_DIRS
      value: /tmp/spark-local
    - name: JOB_NAME
      value: ${job_name}
    - name: SERVICE_ACCOUNT
      value: ${service_account}
    - name: LAKE_FORMATION_AUTHORIZED_CALLER
      value: "$LF_SESSION_TAG_VALUE"
    volumeMounts:
    - name: spark-local-dir
      mountPath: /tmp/spark-local
    - name: spark-conf-volume
      mountPath: /opt/spark/conf
    securityContext:
      runAsUser: 999
      runAsGroup: 1000
      fsGroup: 1000
      runAsNonRoot: true
  volumes:
  - name: spark-local-dir
    emptyDir:
      sizeLimit: 2Gi
  - name: spark-conf-volume
    emptyDir: {}
  terminationGracePeriodSeconds: 30
  dnsPolicy: ClusterFirst
  schedulerName: default-scheduler
EOF
    
    echo "   âœ… Pod Template ìƒì„± ì™„ë£Œ: $pod_template_file"
    return 0
}

# Job ì‹¤í–‰ í•¨ìˆ˜ (Template ì‚¬ìš©)
run_emr_job_with_template() {
    local job_name=$1
    local service_account=$2
    local role_name=$3
    local description=$4
    
    echo ""
    echo "ğŸš€ $job_name Job ì‹¤í–‰ ì¤‘ (Blueprint Template ì‚¬ìš©)..."
    echo "   ì„¤ëª…: $description"
    echo "   ì„œë¹„ìŠ¤ ê³„ì •: $service_account"
    echo "   IAM ì—­í• : $role_name"
    
    # Job Templateê³¼ Pod Template ìƒì„±
    create_job_template "$job_name" "$service_account" "$role_name"
    create_pod_template "$job_name" "$service_account"
    
    # Job Template íŒŒì¼ ì½ê¸°
    local job_template_file="job-templates/${job_name}-job-template.json"
    
    if [ ! -f "$job_template_file" ]; then
        echo "   âŒ Job Template íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $job_template_file"
        return 1
    fi
    
    # Job ì‹¤í–‰ (Template íŒŒì¼ ì‚¬ìš©)
    echo "   ğŸ“‹ Job Template íŒŒì¼ ì‚¬ìš©: $job_template_file"
    
    JOB_ID=$(aws emr-containers start-job-run \
        --region $REGION \
        --cli-input-json file://"$job_template_file" \
        --query 'id' \
        --output text)
    
    if [ -n "$JOB_ID" ] && [ "$JOB_ID" != "None" ]; then
        echo "   âœ… Job ì‹œì‘ë¨: $JOB_ID"
        echo "   ğŸ“Š ëª¨ë‹ˆí„°ë§: aws emr-containers describe-job-run --region $REGION --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID --id $JOB_ID"
        echo "   ğŸ“ Job Template: $job_template_file"
        echo "   ğŸ“ Pod Template: pod-templates/${job_name}-pod-template.yaml"
        echo "   ğŸ” Lake Formation FGAC: í™œì„±í™”ë¨ (Session Tag: $LF_SESSION_TAG_VALUE)"
        
        # Job IDë¥¼ íŒŒì¼ì— ì €ì¥
        echo "$job_name:$JOB_ID:$job_template_file" >> /tmp/emr-job-ids.txt
        
        return 0
    else
        echo "   âŒ Job ì‹œì‘ ì‹¤íŒ¨"
        return 1
    fi
}

# ëª¨ë“  Job ì‹¤í–‰
echo "1. EMR Job Template ë° Pod Template ìƒì„± ë° ì‹¤í–‰..."
rm -f /tmp/emr-job-ids.txt

for job_config in "${JOB_CONFIGS[@]}"; do
    IFS=':' read -r job_name service_account role_name description <<< "$job_config"
    
    if run_emr_job_with_template "$job_name" "$service_account" "$role_name" "$description"; then
        echo "   Job ì‹¤í–‰ ì„±ê³µ: $job_name"
    else
        echo "   Job ì‹¤í–‰ ì‹¤íŒ¨: $job_name"
    fi
    
    # Job ê°„ ê°„ê²©
    sleep 5
done

# Job ìƒíƒœ ëª¨ë‹ˆí„°ë§
echo ""
echo "2. Job ìƒíƒœ ëª¨ë‹ˆí„°ë§..."

if [ -f "/tmp/emr-job-ids.txt" ]; then
    echo ""
    echo "ğŸ“Š ì‹¤í–‰ëœ Job ëª©ë¡ (Blueprint Template ì‚¬ìš©):"
    echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
    echo "â”‚ Job ì´ë¦„                â”‚ Job ID                              â”‚ Template íŒŒì¼                       â”‚"
    echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
    
    while IFS=':' read -r job_name job_id template_file; do
        printf "â”‚ %-23s â”‚ %-35s â”‚ %-35s â”‚\n" "$job_name" "$job_id" "$(basename "$template_file")"
    done < /tmp/emr-job-ids.txt
    
    echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
    
    echo ""
    echo "3. Job ìƒíƒœ í™•ì¸ ì¤‘..."
    
    # ê° Jobì˜ ìƒíƒœ í™•ì¸
    while IFS=':' read -r job_name job_id template_file; do
        echo ""
        echo "   $job_name Job ìƒíƒœ í™•ì¸ ì¤‘..."
        
        # ìµœëŒ€ 2.5ë¶„ ëŒ€ê¸° (5íšŒ retry)
        for i in {1..5}; do
            JOB_STATE=$(aws emr-containers describe-job-run \
                --region $REGION \
                --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID \
                --id $job_id \
                --query 'jobRun.state' \
                --output text 2>/dev/null || echo "UNKNOWN")
            
            case $JOB_STATE in
                "COMPLETED")
                    echo "   âœ… $job_name: ì™„ë£Œ"
                    break
                    ;;
                "FAILED"|"CANCELLED")
                    echo "   âŒ $job_name: ì‹¤íŒ¨ ($JOB_STATE)"
                    
                    # ì‹¤íŒ¨ ì›ì¸ ì¡°íšŒ
                    FAILURE_REASON=$(aws emr-containers describe-job-run \
                        --region $REGION \
                        --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID \
                        --id $job_id \
                        --query 'jobRun.failureReason' \
                        --output text 2>/dev/null || echo "Unknown")
                    
                    echo "      ì‹¤íŒ¨ ì›ì¸: $FAILURE_REASON"
                    break
                    ;;
                "RUNNING"|"PENDING"|"SUBMITTED")
                    echo "   â³ $job_name: ì§„í–‰ ì¤‘ ($JOB_STATE) - ${i}/5"
                    if [ $i -lt 5 ]; then
                        sleep 30
                    fi
                    ;;
                *)
                    echo "   â“ $job_name: ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ ($JOB_STATE)"
                    break
                    ;;
            esac
        done
        
        # ìµœì¢… ìƒíƒœê°€ RUNNINGì´ë©´ íƒ€ì„ì•„ì›ƒ ë©”ì‹œì§€
        if [ "$JOB_STATE" = "RUNNING" ] || [ "$JOB_STATE" = "PENDING" ]; then
            echo "   â° $job_name: íƒ€ì„ì•„ì›ƒ (ì—¬ì „íˆ ì‹¤í–‰ ì¤‘)"
        fi
        
    done < /tmp/emr-job-ids.txt
    
    echo ""
    echo "4. Job ê²°ê³¼ ë° Template ì •ë³´..."
    
    # Template íŒŒì¼ ìœ„ì¹˜ ì•ˆë‚´
    echo ""
    echo "ğŸ“ ìƒì„±ëœ Template íŒŒì¼:"
    echo "   Job Templates: ./job-templates/"
    echo "   Pod Templates: ./pod-templates/"
    ls -la job-templates/ | grep -E "\.json$" | awk '{print "     - " $9}'
    ls -la pod-templates/ | grep -E "\.yaml$" | awk '{print "     - " $9}'
    
    # ë¡œê·¸ ìœ„ì¹˜ ì•ˆë‚´
    echo ""
    echo "ğŸ“ Job ë¡œê·¸ ìœ„ì¹˜:"
    echo "   S3 ë²„í‚·: s3://$RESULTS_BUCKET/logs/"
    echo "   ë¡œì»¬ í™•ì¸: aws s3 ls s3://$RESULTS_BUCKET/logs/ --recursive"
    
    # Job ìƒì„¸ ì •ë³´ ì¡°íšŒ ëª…ë ¹ì–´ ì•ˆë‚´
    echo ""
    echo "ğŸ” Job ìƒì„¸ ì •ë³´ ì¡°íšŒ ëª…ë ¹ì–´:"
    while IFS=':' read -r job_name job_id template_file; do
        echo "   $job_name: aws emr-containers describe-job-run --region $REGION --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID --id $job_id"
    done < /tmp/emr-job-ids.txt
    
else
    echo "âŒ ì‹¤í–‰ëœ Jobì´ ì—†ìŠµë‹ˆë‹¤."
fi

# ì„ì‹œ íŒŒì¼ ì •ë¦¬
rm -f /tmp/emr-job-ids.txt

echo ""
echo "=== EMR on EKS Job ì‹¤í–‰ ì™„ë£Œ (Lake Formation FGAC + S3 Tables) ==="
echo ""
echo "ğŸ“‹ ì‹¤í–‰ ìš”ì•½:"
echo "   â€¢ ì´ 4ê°œ ì—­í• ë³„ ë¶„ì„ Job ì‹¤í–‰"
echo "   â€¢ Lake Formation FGAC Virtual Cluster ì‚¬ìš©: $LF_VIRTUAL_CLUSTER_ID"
echo "   â€¢ Security Configuration ì ìš©: $SECURITY_CONFIG_ID"
echo "   â€¢ Session Tag ì„¤ì •: LakeFormationAuthorizedCaller=$LF_SESSION_TAG_VALUE"
echo "   â€¢ S3 Tables (Apache Iceberg) ì¹´íƒˆë¡œê·¸: s3tablescatalog"
echo "   â€¢ EMR on EKS Blueprint Job Template ë° Pod Template í™œìš©"
echo "   â€¢ S3 Tables ë°ì´í„° ë¶„ì„ (ê¸°ë³¸ ì¹´íƒˆë¡œê·¸: s3tablescatalog)"
echo "   â€¢ Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™œì„±í™”"
echo "   â€¢ Karpenter ê¸°ë°˜ ë…¸ë“œ ìŠ¤ì¼€ì¤„ë§"
echo ""
echo "ğŸ—‚ï¸ S3 Tables ì¹´íƒˆë¡œê·¸ ì„¤ì •:"
echo "   â€¢ ì¹´íƒˆë¡œê·¸ëª…: s3tablescatalog"
echo "   â€¢ ê¸°ë³¸ ì¹´íƒˆë¡œê·¸: spark.sql.defaultCatalog=s3tablescatalog"
echo "   â€¢ í…Œì´ë¸” ì°¸ì¡°: bike_db.bike_rental_data"
echo "   â€¢ Warehouse: arn:aws:s3tables:$REGION:$ACCOUNT_ID:bucket/${TABLE_BUCKET_NAME}"
echo ""
echo "ğŸ” Lake Formation FGAC ì ìš© ê²°ê³¼:"
echo "   â€¢ DataSteward: 100,000ê±´ ì „ì²´ ë¶„ì„ (ëª¨ë“  ì»¬ëŸ¼ ì ‘ê·¼)"
echo "   â€¢ GangnamAnalytics: ~3,000ê±´ (ê°•ë‚¨êµ¬ë§Œ, birth_year ì œì™¸)"
echo "   â€¢ Operation: 100,000ê±´ (ê°œì¸ì •ë³´ ì œì™¸: birth_year, gender)"
echo "   â€¢ MarketingPartner: ~1,650ê±´ (ê°•ë‚¨êµ¬ 20-30ëŒ€ë§Œ, birth_year ì œì™¸)"
echo ""
echo "ğŸ“ Template ì¬ì‚¬ìš©:"
echo "   â€¢ Job Templates: ./job-templates/ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥"
echo "   â€¢ Pod Templates: ./pod-templates/ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥"
echo "   â€¢ í–¥í›„ ìœ ì‚¬í•œ Job ì‹¤í–‰ ì‹œ Template ìˆ˜ì •í•˜ì—¬ í™œìš©"
echo ""
echo "ğŸ¯ Lake Formation FGAC + S3 Tables ê²€ì¦:"
echo "   â€¢ ê° ì—­í• ë³„ë¡œ ë‹¤ë¥¸ ë°ì´í„° ì ‘ê·¼ ê²°ê³¼ í™•ì¸"
echo "   â€¢ Row-level Security: ì§€ì—­ë³„ í•„í„°ë§ (ê°•ë‚¨êµ¬)"
echo "   â€¢ Column-level Security: ì—­í• ë³„ ì»¬ëŸ¼ ì ‘ê·¼ ì œì–´"
echo "   â€¢ Cell-level Security: ì—°ë ¹ëŒ€ë³„ ì„¸ë°€í•œ ì œì–´ (20-30ëŒ€)"
echo "   â€¢ S3 Tables ë„¤ì´í‹°ë¸Œ ì¹´íƒˆë¡œê·¸ í™œìš©"
echo ""
echo "âœ… ë‹¤ìŒ ë‹¨ê³„: ./scripts/06-verify-and-analyze.sh"
