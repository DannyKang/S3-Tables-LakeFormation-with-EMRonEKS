#!/usr/bin/env python3
"""
Marketing Partner Role - ê°•ë‚¨êµ¬ 20ëŒ€ íƒ€ê²Ÿ ë§ˆì¼€íŒ… ë¶„ì„
ëª©ì : ê°•ë‚¨êµ¬ 20ëŒ€ ê³ ê° ëŒ€ìƒ ë§ˆì¼€íŒ… ìº í˜ì¸ ê¸°íš ë° íƒ€ê²Ÿ ë¶„ì„
ì œí•œ: district='ê°•ë‚¨êµ¬' AND age_group='20ëŒ€' ë°ì´í„°ë§Œ ì ‘ê·¼, 
      payment_amount/user_id/rental_duration ì»¬ëŸ¼ ì ‘ê·¼ ë¶ˆê°€
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def create_spark_session():
    """Lake Formation í†µí•© Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \
        .appName("MarketingPartner-SeoulBikeAnalysis") \
        .config("spark.sql.catalog.seoul_bike_catalog", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.seoul_bike_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog") \
        .config("spark.sql.catalog.seoul_bike_catalog.warehouse", "s3://seoul-bike-rental-data-202506/") \
        .config("spark.sql.catalog.seoul_bike_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.WebIdentityTokenCredentialsProvider") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .getOrCreate()

def analyze_target_demographics(spark):
    """ê°•ë‚¨êµ¬ 20ëŒ€ íƒ€ê²Ÿ ê³ ê° ë¶„ì„"""
    print("=== ê°•ë‚¨êµ¬ 20ëŒ€ íƒ€ê²Ÿ ê³ ê° ë¶„ì„ ===")
    
    # Lake Formation FGACì— ì˜í•´ ê°•ë‚¨êµ¬ 20ëŒ€ ë°ì´í„°ë§Œ ìë™ í•„í„°ë§ë¨
    df = spark.sql("""
        SELECT rental_id, station_id, station_name, district, rental_date, return_date,
               user_type, age_group, gender
        FROM seoul_bike_catalog.bike_db.bike_rental_data
        WHERE rental_date >= '2025-06-01' AND rental_date <= '2025-06-30'
    """)
    
    # ë°ì´í„° í™•ì¸ - ê°•ë‚¨êµ¬ 20ëŒ€ ë°ì´í„°ë§Œ ë³´ì—¬ì•¼ í•¨
    print("ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„° í™•ì¸:")
    df.select("district", "age_group").distinct().show()
    
    total_target_customers = df.count()
    print(f"íƒ€ê²Ÿ ê³ ê° ëŒ€ì—¬ ê±´ìˆ˜: {total_target_customers:,}")
    
    # ì„±ë³„ ë¶„í¬ ë¶„ì„
    gender_analysis = df.groupBy("gender") \
        .agg(count("*").alias("rental_count")) \
        .withColumn("percentage", round((col("rental_count") * 100.0 / total_target_customers), 2)) \
        .orderBy(desc("rental_count"))
    
    print("\n=== íƒ€ê²Ÿ ê³ ê° ì„±ë³„ ë¶„í¬ ===")
    gender_analysis.show()
    
    return df

def analyze_station_preferences(df):
    """ì •ê±°ì¥ë³„ ì´ìš© ì„ í˜¸ë„ ë¶„ì„"""
    print("\n=== ê°•ë‚¨êµ¬ 20ëŒ€ ì •ê±°ì¥ ì´ìš© ì„ í˜¸ë„ ===")
    
    # ì •ê±°ì¥ë³„ ì´ìš© í˜„í™©
    station_preference = df.groupBy("station_id", "station_name") \
        .agg(
            count("*").alias("usage_count"),
            countDistinct("rental_date").alias("usage_days")
        ) \
        .orderBy(desc("usage_count"))
    
    print("ì •ê±°ì¥ë³„ ì´ìš© í˜„í™©:")
    station_preference.show(20, truncate=False)
    
    # ì„±ë³„ ì •ê±°ì¥ ì„ í˜¸ë„
    gender_station_preference = df.groupBy("station_name", "gender") \
        .agg(count("*").alias("usage_count")) \
        .orderBy("station_name", desc("usage_count"))
    
    print("ì„±ë³„ ì •ê±°ì¥ ì„ í˜¸ë„:")
    gender_station_preference.show(30, truncate=False)
    
    return station_preference

def analyze_usage_patterns(df):
    """ì´ìš© íŒ¨í„´ ë¶„ì„"""
    print("\n=== ê°•ë‚¨êµ¬ 20ëŒ€ ì´ìš© íŒ¨í„´ ë¶„ì„ ===")
    
    # ì‚¬ìš©ì ìœ í˜•ë³„ ë¶„ì„
    user_type_analysis = df.groupBy("user_type", "gender") \
        .agg(count("*").alias("usage_count")) \
        .orderBy("user_type", desc("usage_count"))
    
    print("ì‚¬ìš©ì ìœ í˜•ë³„ ì„±ë³„ ë¶„í¬:")
    user_type_analysis.show()
    
    # ì‹œê°„ëŒ€ë³„ ì´ìš© íŒ¨í„´
    hourly_pattern = df.withColumn("hour", hour("rental_date")) \
        .groupBy("hour", "gender") \
        .agg(count("*").alias("usage_count")) \
        .orderBy("hour", "gender")
    
    print("ì‹œê°„ëŒ€ë³„ ì„±ë³„ ì´ìš© íŒ¨í„´:")
    hourly_pattern.show(48)
    
    # ìš”ì¼ë³„ íŒ¨í„´
    df_with_weekday = df.withColumn("weekday", date_format("rental_date", "EEEE")) \
                       .withColumn("day_of_week", dayofweek("rental_date"))
    
    weekday_pattern = df_with_weekday.groupBy("day_of_week", "weekday", "gender") \
        .agg(count("*").alias("usage_count")) \
        .orderBy("day_of_week", "gender")
    
    print("ìš”ì¼ë³„ ì„±ë³„ ì´ìš© íŒ¨í„´:")
    weekday_pattern.show()

def generate_marketing_insights(df):
    """ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    print("\n=== ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ë° ìº í˜ì¸ ì œì•ˆ ===")
    
    # í”¼í¬ ì‹œê°„ëŒ€ ë¶„ì„
    peak_hours = df.withColumn("hour", hour("rental_date")) \
        .groupBy("hour") \
        .agg(count("*").alias("usage_count")) \
        .orderBy(desc("usage_count"))
    
    print("í”¼í¬ ì‹œê°„ëŒ€ TOP 5:")
    peak_hours.show(5)
    
    # ì¸ê¸° ì •ê±°ì¥ TOP 5
    popular_stations = df.groupBy("station_name") \
        .agg(count("*").alias("usage_count")) \
        .orderBy(desc("usage_count"))
    
    print("ì¸ê¸° ì •ê±°ì¥ TOP 5:")
    popular_stations.show(5, truncate=False)
    
    # ì‚¬ìš©ì ìœ í˜•ë³„ ì„ í˜¸ ì‹œê°„ëŒ€
    user_time_preference = df.withColumn("hour", hour("rental_date")) \
        .withColumn("time_period",
            when(col("hour").between(6, 11), "ì˜¤ì „")
            .when(col("hour").between(12, 17), "ì˜¤í›„")
            .when(col("hour").between(18, 23), "ì €ë…")
            .otherwise("ìƒˆë²½")
        ) \
        .groupBy("user_type", "time_period", "gender") \
        .agg(count("*").alias("usage_count")) \
        .orderBy("user_type", desc("usage_count"))
    
    print("ì‚¬ìš©ì ìœ í˜•ë³„ ì‹œê°„ëŒ€ ì„ í˜¸ë„:")
    user_time_preference.show(20)

def generate_campaign_recommendations(df):
    """ìº í˜ì¸ ì¶”ì²œì•ˆ ìƒì„±"""
    print("\n=== ë§ˆì¼€íŒ… ìº í˜ì¸ ì¶”ì²œì•ˆ ===")
    
    # ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìˆ˜ì§‘
    total_rentals = df.count()
    unique_stations = df.select("station_id").distinct().count()
    
    # ì„±ë³„ ë¶„í¬
    gender_dist = df.groupBy("gender").count().collect()
    male_count = next((row['count'] for row in gender_dist if row['gender'] == 'M'), 0)
    female_count = next((row['count'] for row in gender_dist if row['gender'] == 'F'), 0)
    
    # ì‚¬ìš©ì ìœ í˜• ë¶„í¬
    user_type_dist = df.groupBy("user_type").count().collect()
    regular_count = next((row['count'] for row in user_type_dist if row['user_type'] == 'ì •ê¸°'), 0)
    casual_count = next((row['count'] for row in user_type_dist if row['user_type'] == 'ì¼ë°˜'), 0)
    tourist_count = next((row['count'] for row in user_type_dist if row['user_type'] == 'ê´€ê´‘ê°'), 0)
    
    print("ğŸ“Š íƒ€ê²Ÿ ê³ ê° í”„ë¡œí•„ ìš”ì•½:")
    print(f"   â€¢ ì´ ëŒ€ì—¬ ê±´ìˆ˜: {total_rentals}ê±´")
    print(f"   â€¢ ì´ìš© ì •ê±°ì¥ ìˆ˜: {unique_stations}ê°œ")
    print(f"   â€¢ ë‚¨ì„±: {male_count}ê±´ ({male_count/total_rentals*100:.1f}%)")
    print(f"   â€¢ ì—¬ì„±: {female_count}ê±´ ({female_count/total_rentals*100:.1f}%)")
    print(f"   â€¢ ì •ê¸° ì´ìš©ì: {regular_count}ê±´")
    print(f"   â€¢ ì¼ë°˜ ì´ìš©ì: {casual_count}ê±´")
    print(f"   â€¢ ê´€ê´‘ê°: {tourist_count}ê±´")
    
    print("\nğŸ¯ ì¶”ì²œ ë§ˆì¼€íŒ… ìº í˜ì¸:")
    
    if male_count > female_count:
        print("1. ë‚¨ì„± íƒ€ê²Ÿ ìº í˜ì¸")
        print("   â€¢ 'ê°•ë‚¨ ì§ì¥ì¸ ë‚¨ì„±ì„ ìœ„í•œ ì¶œí‡´ê·¼ ìì „ê±° íŒ¨í‚¤ì§€'")
        print("   â€¢ ì˜¤ì „/ì €ë… ì‹œê°„ëŒ€ ì§‘ì¤‘ í”„ë¡œëª¨ì…˜")
    else:
        print("1. ì—¬ì„± íƒ€ê²Ÿ ìº í˜ì¸")
        print("   â€¢ 'ê°•ë‚¨ ì—¬ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ìì „ê±° ë¼ì´ë”©'")
        print("   â€¢ ì£¼ë§ ë ˆì € í™œë™ ì—°ê³„ í”„ë¡œëª¨ì…˜")
    
    if regular_count > casual_count:
        print("\n2. ì •ê¸° ì´ìš©ì í™•ëŒ€ ìº í˜ì¸")
        print("   â€¢ '20ëŒ€ ì •ê¸° ì´ìš©ê¶Œ í• ì¸ ì´ë²¤íŠ¸'")
        print("   â€¢ ì¹œêµ¬ ì¶”ì²œ ì‹œ ì¶”ê°€ í˜œíƒ ì œê³µ")
    else:
        print("\n2. ì¼ë°˜ ì´ìš©ì ì „í™˜ ìº í˜ì¸")
        print("   â€¢ 'ì²« ì •ê¸°ê¶Œ 50% í• ì¸ ì´ë²¤íŠ¸'")
        print("   â€¢ ë¬´ë£Œ ì²´í—˜ ê¸°ê°„ ì œê³µ")
    
    if tourist_count > 0:
        print("\n3. ê´€ê´‘ê° íƒ€ê²Ÿ ìº í˜ì¸")
        print("   â€¢ 'ê°•ë‚¨ íˆ¬ì–´ ìì „ê±° íŒ¨í‚¤ì§€'")
        print("   â€¢ ê´€ê´‘ ëª…ì†Œ ì—°ê³„ í• ì¸ í˜œíƒ")
    
    print("\nğŸ“ ì¶”ì²œ ê´‘ê³  ìœ„ì¹˜:")
    top_stations = df.groupBy("station_name").count().orderBy(desc("count")).limit(3).collect()
    for i, station in enumerate(top_stations, 1):
        print(f"   {i}. {station['station_name']} ({station['count']}ê±´)")

def test_restricted_access(spark):
    """ì œí•œëœ ì»¬ëŸ¼ ì ‘ê·¼ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì œí•œëœ ì»¬ëŸ¼ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ===")
    
    # ê²°ì œ ì •ë³´ ì ‘ê·¼ ì‹œë„ (ì‹¤íŒ¨í•´ì•¼ í•¨)
    print("1. ê²°ì œ ì •ë³´ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨ ì˜ˆìƒ):")
    try:
        payment_data = spark.sql("""
            SELECT station_name, sum(payment_amount) as total_revenue
            FROM seoul_bike_catalog.bike_db.bike_rental_data
            GROUP BY station_name
            LIMIT 10
        """)
        payment_data.show()
        print("âš ï¸  ê²½ê³ : ê²°ì œ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œí•œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âœ… ì˜ˆìƒëœ ê²°ê³¼: ê²°ì œ ì •ë³´ ì ‘ê·¼ ì°¨ë‹¨ë¨ - {str(e)}")
    
    # ê°œì¸ì •ë³´ ì ‘ê·¼ ì‹œë„ (ì‹¤íŒ¨í•´ì•¼ í•¨)
    print("\n2. ê°œì¸ì •ë³´ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨ ì˜ˆìƒ):")
    try:
        user_data = spark.sql("""
            SELECT user_id, count(*) as rental_count
            FROM seoul_bike_catalog.bike_db.bike_rental_data
            GROUP BY user_id
            LIMIT 10
        """)
        user_data.show()
        print("âš ï¸  ê²½ê³ : ê°œì¸ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œí•œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âœ… ì˜ˆìƒëœ ê²°ê³¼: ê°œì¸ì •ë³´ ì ‘ê·¼ ì°¨ë‹¨ë¨ - {str(e)}")
    
    # ëŒ€ì—¬ ì‹œê°„ ì •ë³´ ì ‘ê·¼ ì‹œë„ (ì‹¤íŒ¨í•´ì•¼ í•¨)
    print("\n3. ëŒ€ì—¬ ì‹œê°„ ì •ë³´ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨ ì˜ˆìƒ):")
    try:
        duration_data = spark.sql("""
            SELECT station_name, avg(rental_duration) as avg_duration
            FROM seoul_bike_catalog.bike_db.bike_rental_data
            GROUP BY station_name
            LIMIT 10
        """)
        duration_data.show()
        print("âš ï¸  ê²½ê³ : ëŒ€ì—¬ ì‹œê°„ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œí•œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âœ… ì˜ˆìƒëœ ê²°ê³¼: ëŒ€ì—¬ ì‹œê°„ ì •ë³´ ì ‘ê·¼ ì°¨ë‹¨ë¨ - {str(e)}")

def main():
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        print("Marketing Partner Role - ê°•ë‚¨êµ¬ 20ëŒ€ íƒ€ê²Ÿ ë§ˆì¼€íŒ… ë¶„ì„ ì‹œì‘")
        print("ğŸ¯ íƒ€ê²Ÿ: ê°•ë‚¨êµ¬ ê±°ì£¼/ì´ìš© 20ëŒ€ ê³ ê°")
        print("ğŸ“Š ëª©ì : íƒ€ê²Ÿ ë§ˆì¼€íŒ… ìº í˜ì¸ ê¸°íš ë° ê³ ê° ë¶„ì„")
        print("ğŸ”’ ì œí•œ: Lake Formation FGACì— ì˜í•´ ê°•ë‚¨êµ¬ 20ëŒ€ ë°ì´í„°ë§Œ ì ‘ê·¼ ê°€ëŠ¥")
        
        # íƒ€ê²Ÿ ê³ ê° ë¶„ì„
        df = analyze_target_demographics(spark)
        
        # ì •ê±°ì¥ ì„ í˜¸ë„ ë¶„ì„
        station_pref = analyze_station_preferences(df)
        
        # ì´ìš© íŒ¨í„´ ë¶„ì„
        analyze_usage_patterns(df)
        
        # ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ìƒì„±
        generate_marketing_insights(df)
        
        # ìº í˜ì¸ ì¶”ì²œì•ˆ ìƒì„±
        generate_campaign_recommendations(df)
        
        # ì œí•œëœ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        test_restricted_access(spark)
        
        # ê²°ê³¼ ì €ì¥
        output_path = "s3://seoul-bike-analytics-results/marketing-partner/monthly-report-202506/"
        
        # ë§ˆì¼€íŒ… íƒ€ê²Ÿ ìš”ì•½ ì €ì¥
        marketing_summary = df.groupBy("station_id", "station_name", "user_type", "gender") \
            .agg(count("*").alias("target_count")) \
            .orderBy(desc("target_count"))
        
        marketing_summary.coalesce(1) \
            .write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(f"{output_path}/marketing_target_summary")
        
        print(f"\nğŸ“ ë§ˆì¼€íŒ… ë¶„ì„ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìµœì¢… ìš”ì•½
        print("\n" + "="*60)
        print("ğŸ¯ ë§ˆì¼€íŒ… íŒŒíŠ¸ë„ˆ ë¶„ì„ ì™„ë£Œ")
        print("="*60)
        print("âœ… ê°•ë‚¨êµ¬ 20ëŒ€ íƒ€ê²Ÿ ê³ ê° í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ")
        print("âœ… ì •ê±°ì¥ë³„ ì´ìš© ì„ í˜¸ë„ ë¶„ì„ ì™„ë£Œ")
        print("âœ… ì‹œê°„ëŒ€/ìš”ì¼ë³„ ì´ìš© íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        print("âœ… ë§ì¶¤í˜• ë§ˆì¼€íŒ… ìº í˜ì¸ ì¶”ì²œì•ˆ ìƒì„± ì™„ë£Œ")
        print("âœ… ë°ì´í„° ì ‘ê·¼ ì œí•œ ê²€ì¦ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
