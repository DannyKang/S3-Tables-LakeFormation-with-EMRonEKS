#!/usr/bin/env python3
"""
Operation Analysis - ìš´ì˜ ë°ì´í„° ë¶„ì„
Lake Formation FGAC ë°ëª¨ìš© - ì „ì²´ êµ¬ ìš´ì˜ ë°ì´í„° (ê°œì¸ì •ë³´ ì œì™¸)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def create_spark_session():
    """S3 Tables(Iceberg) ì§€ì› Spark ì„¸ì…˜ ìƒì„±"""
    import os
    table_bucket = os.getenv('TABLE_BUCKET_NAME', 'seoul-bike-demo-2025')
    
    return SparkSession.builder \
        .appName("Operation-SystemAccess-Analysis") \
        .config("spark.sql.catalog.s3tables_catalog", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.s3tables_catalog.catalog-impl", "org.apache.iceberg.aws.s3.S3TablesCatalog") \
        .config("spark.sql.catalog.s3tables_catalog.warehouse", f"s3://{table_bucket}/") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .getOrCreate()

def main():
    print("=== Operation Analysis ì‹œì‘ ===")
    print("ì—­í• : ìš´ì˜íŒ€ - ì „ì²´ êµ¬ ìš´ì˜ ë°ì´í„° (ê°œì¸ì •ë³´ ì œì™¸)")
    
    spark = create_spark_session()
    
    try:
        # S3 Tablesì—ì„œ ë°ì´í„° ì½ê¸°
        print("\n1. S3 Tablesì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        spark_conf = spark.sparkContext.getConf()
        namespace = spark_conf.get('spark.app.s3tables.namespace', 'bike_db')
        table_name = spark_conf.get('spark.app.s3tables.table', 'bike_rental_data')
        
        print(f"   ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace}")
        print(f"   í…Œì´ë¸”ëª…: {table_name}")
        
        df = spark.read.table(f"s3tables_catalog.{namespace}.{table_name}")
        
        total_records = df.count()
        print(f"âœ… Lake Formation í•„í„°ë§ í›„ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê±´")
        print("ğŸ”’ ì ‘ê·¼ ì œí•œ: ê°œì¸ì •ë³´(birth_year, gender) ì œì™¸")
        
        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ í™•ì¸ (ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ)
        print("\n2. ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„° ìŠ¤í‚¤ë§ˆ:")
        df.printSchema()
        
        # êµ¬ë³„ ìš´ì˜ í˜„í™©
        print(f"\n3. êµ¬ë³„ ìš´ì˜ í˜„í™©:")
        district_ops = df.groupBy("district") \
                         .agg(count("*").alias("ì´_ì´ìš©íšŸìˆ˜"),
                              countDistinct("station_id").alias("ì •ê±°ì¥_ìˆ˜"),
                              avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„"),
                              avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬")) \
                         .orderBy(desc("ì´_ì´ìš©íšŸìˆ˜"))
        
        district_ops.show(truncate=False)
        
        # ì •ê±°ì¥ë³„ ìš´ì˜ íš¨ìœ¨ì„± ë¶„ì„
        print(f"\n4. ì •ê±°ì¥ë³„ ìš´ì˜ íš¨ìœ¨ì„± (ìƒìœ„ 20ê°œ):")
        station_efficiency = df.groupBy("station_id", "station_name", "district") \
                               .agg(count("*").alias("ì´ìš©íšŸìˆ˜"),
                                    avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„"),
                                    avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬"),
                                    min("usage_min").alias("ìµœì†Œ_ì´ìš©ì‹œê°„"),
                                    max("usage_min").alias("ìµœëŒ€_ì´ìš©ì‹œê°„")) \
                               .orderBy(desc("ì´ìš©íšŸìˆ˜")) \
                               .limit(20)
        
        station_efficiency.show(truncate=False)
        
        # ì‹œê°„ëŒ€ë³„ ìš´ì˜ ë¶€í•˜ ë¶„ì„
        print(f"\n5. ì‹œê°„ëŒ€ë³„ ìš´ì˜ ë¶€í•˜:")
        hourly_load = df.withColumn("hour", hour("rental_date")) \
                        .groupBy("hour") \
                        .agg(count("*").alias("ì´ìš©íšŸìˆ˜"),
                             avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„"),
                             countDistinct("station_id").alias("í™œì„±_ì •ê±°ì¥ìˆ˜")) \
                        .orderBy("hour")
        
        hourly_load.show(24)
        
        # ìš”ì¼ë³„ ìš´ì˜ íŒ¨í„´
        print(f"\n6. ìš”ì¼ë³„ ìš´ì˜ íŒ¨í„´:")
        weekday_ops = df.withColumn("weekday", date_format("rental_date", "EEEE")) \
                        .groupBy("weekday") \
                        .agg(count("*").alias("ì´ìš©íšŸìˆ˜"),
                             countDistinct("station_id").alias("í™œì„±_ì •ê±°ì¥ìˆ˜"),
                             avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„"),
                             avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬")) \
                        .orderBy(desc("ì´ìš©íšŸìˆ˜"))
        
        weekday_ops.show()
        
        # ì´ìš© ì‹œê°„ ë¶„í¬ (ìš´ì˜ ìµœì í™”ìš©)
        print(f"\n7. ì´ìš© ì‹œê°„ ë¶„í¬ ë¶„ì„:")
        usage_distribution = df.withColumn("usage_category",
                                         when(col("usage_min") <= 5, "ë‹¨ê±°ë¦¬(â‰¤5ë¶„)")
                                         .when(col("usage_min") <= 15, "ì¤‘ê±°ë¦¬(6-15ë¶„)")
                                         .when(col("usage_min") <= 30, "ì¥ê±°ë¦¬(16-30ë¶„)")
                                         .when(col("usage_min") <= 60, "ì´ˆì¥ê±°ë¦¬(31-60ë¶„)")
                                         .otherwise("íŠ¹ìˆ˜(>60ë¶„)")) \
                              .groupBy("usage_category") \
                              .agg(count("*").alias("ì´ìš©íšŸìˆ˜"),
                                   avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬")) \
                              .orderBy(desc("ì´ìš©íšŸìˆ˜"))
        
        usage_distribution.show()
        
        # ì´ë™ ê±°ë¦¬ ë¶„í¬ (ìì „ê±° ì¬ë°°ì¹˜ ê³„íšìš©)
        print(f"\n8. ì´ë™ ê±°ë¦¬ ë¶„í¬ ë¶„ì„:")
        distance_distribution = df.withColumn("distance_category",
                                            when(col("distance_meter") <= 500, "ê·¼ê±°ë¦¬(â‰¤500m)")
                                            .when(col("distance_meter") <= 1500, "ì¤‘ê±°ë¦¬(501-1500m)")
                                            .when(col("distance_meter") <= 3000, "ì¥ê±°ë¦¬(1501-3000m)")
                                            .when(col("distance_meter") <= 5000, "ì´ˆì¥ê±°ë¦¬(3001-5000m)")
                                            .otherwise("íŠ¹ìˆ˜(>5000m)")) \
                                 .groupBy("distance_category") \
                                 .agg(count("*").alias("ì´ìš©íšŸìˆ˜"),
                                      avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„")) \
                                 .orderBy(desc("ì´ìš©íšŸìˆ˜"))
        
        distance_distribution.show()
        
        # ì •ê±°ì¥ ì´ìš©ë¥  ë¶„ì„ (ì¬ë°°ì¹˜ ìš°ì„ ìˆœìœ„)
        print(f"\n9. ì •ê±°ì¥ ì´ìš©ë¥  ë¶„ì„ (í•˜ìœ„ 10ê°œ - ì¬ë°°ì¹˜ í•„ìš”):")
        low_usage_stations = df.groupBy("station_id", "station_name", "district") \
                               .agg(count("*").alias("ì´ìš©íšŸìˆ˜"),
                                    avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„")) \
                               .orderBy("ì´ìš©íšŸìˆ˜") \
                               .limit(10)
        
        low_usage_stations.show(truncate=False)
        
        # ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ
        print(f"\n10. ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ:")
        performance_metrics = df.agg(
            count("*").alias("ì´_ì´ìš©íšŸìˆ˜"),
            countDistinct("station_id").alias("ì´_ì •ê±°ì¥ìˆ˜"),
            countDistinct("district").alias("ì„œë¹„ìŠ¤_êµ¬ìˆ˜"),
            avg("usage_min").alias("í‰ê· _ì´ìš©ì‹œê°„"),
            avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(usage_min, 0.95)").alias("95í¼ì„¼íƒ€ì¼_ì´ìš©ì‹œê°„"),
            expr("percentile_approx(distance_meter, 0.95)").alias("95í¼ì„¼íƒ€ì¼_ì´ë™ê±°ë¦¬")
        )
        
        performance_metrics.show()
        
        print(f"\n=== Operation Analysis ì™„ë£Œ ===")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {total_records:,}ê±´ (ì „ì²´ êµ¬)")
        print(f"ğŸ”’ ê¶Œí•œ: ìš´ì˜ ë°ì´í„°ë§Œ ì ‘ê·¼ (ê°œì¸ì •ë³´ ì œì™¸)")
        print(f"ğŸ“Š ì—­í• : ì‹œìŠ¤í…œ ìš´ì˜ ìµœì í™” ë° ì •ê±°ì¥ ê´€ë¦¬")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
