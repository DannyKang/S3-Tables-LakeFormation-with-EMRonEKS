#!/usr/bin/env python3
"""
Data Steward Analysis - ì „ì²´ ë°ì´í„° ë¶„ì„
Lake Formation FGAC ë°ëª¨ìš© - ì „ì²´ 100,000ê±´ ë°ì´í„° ì ‘ê·¼
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def create_spark_session():
    """S3 Tables Lake Formation FGAC ì§€ì› Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \
        .appName("DataSteward-FullAccess-Analysis") \
        .getOrCreate()

def main():
    print("=== Data Steward Analysis ì‹œì‘ ===")
    print("ì—­í• : ë°ì´í„° ê´€ë¦¬ì - ì „ì²´ ë°ì´í„° ì ‘ê·¼ ê¶Œí•œ")
    
    spark = create_spark_session()
    
    try:
        # S3 Tables Lake Formation FGACë¥¼ í†µí•´ ë°ì´í„° ì½ê¸°
        print("\n1. S3 Tables Lake Formation FGACë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ ì¤‘...")
        namespace = "bike_db"
        table_name = "bike_rental_data"
        
        print(f"   ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace}")
        print(f"   í…Œì´ë¸”ëª…: {table_name}")
        print(f"   ì¹´íƒˆë¡œê·¸: {namespace}.{table_name} (Glue Catalog with Iceberg JAR)")
        
        # S3 Tables ì¹´íƒˆë¡œê·¸ë¥¼ í†µí•´ ë°ì´í„° ì½ê¸° (ë„¤ì´í‹°ë¸Œ S3 Tables ì§€ì›)
        df = spark.read.table(f"{namespace}.{table_name}")
        
        total_records = df.count()
        print(f"âœ… ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê±´")
        
        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ í™•ì¸
        print("\n2. ë°ì´í„° ìŠ¤í‚¤ë§ˆ:")
        df.printSchema()
        
        # ê¸°ë³¸ í†µê³„
        print(f"\n3. ê¸°ë³¸ í†µê³„ ì •ë³´:")
        print(f"   â€¢ ê³ ìœ  ëŒ€ì—¬ ID: {df.select('rental_id').distinct().count():,}")
        print(f"   â€¢ ê³ ìœ  ì •ê±°ì¥: {df.select('station_id').distinct().count():,}")
        print(f"   â€¢ ê³ ìœ  êµ¬: {df.select('district').distinct().count()}")
        
        # êµ¬ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ)
        print(f"\n4. êµ¬ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        district_stats = df.groupBy("district") \
                           .agg(count("*").alias("count"),
                                avg("usage_min").alias("avg_usage"),
                                avg("distance_meter").alias("avg_distance")) \
                           .orderBy(desc("count")) \
                           .limit(10)
        
        district_stats.show(truncate=False)
        
        # ì„±ë³„ ë¶„í¬
        print(f"\n5. ì„±ë³„ ë¶„í¬:")
        gender_stats = df.groupBy("gender") \
                         .agg(count("*").alias("count"),
                              avg("usage_min").alias("avg_usage")) \
                         .orderBy(desc("count"))
        
        gender_stats.show()
        
        # ì—°ë ¹ëŒ€ë³„ ë¶„í¬ (ê°œì¸ì •ë³´ ì ‘ê·¼ ê°€ëŠ¥)
        print(f"\n6. ì—°ë ¹ëŒ€ë³„ ë¶„í¬:")
        age_group_df = df.withColumn("age_group", 
                                   when(col("birth_year") >= 2005, "10ëŒ€")
                                   .when(col("birth_year") >= 1995, "20ëŒ€")
                                   .when(col("birth_year") >= 1985, "30ëŒ€")
                                   .when(col("birth_year") >= 1975, "40ëŒ€")
                                   .when(col("birth_year") >= 1965, "50ëŒ€")
                                   .otherwise("60ëŒ€+"))
        
        age_stats = age_group_df.groupBy("age_group") \
                               .agg(count("*").alias("count"),
                                    avg("usage_min").alias("avg_usage"),
                                    avg("distance_meter").alias("avg_distance")) \
                               .orderBy("age_group")
        
        age_stats.show()
        
        # ëŒ€ì—¬ ì‹œê°„ í†µê³„
        print(f"\n7. ëŒ€ì—¬ ì‹œê°„ í†µê³„:")
        usage_stats = df.select(
            avg("usage_min").alias("í‰ê· _ëŒ€ì—¬ì‹œê°„"),
            min("usage_min").alias("ìµœì†Œ_ëŒ€ì—¬ì‹œê°„"),
            max("usage_min").alias("ìµœëŒ€_ëŒ€ì—¬ì‹œê°„"),
            expr("percentile_approx(usage_min, 0.5)").alias("ì¤‘ì•™ê°’_ëŒ€ì—¬ì‹œê°„")
        )
        
        usage_stats.show()
        
        # ì´ë™ ê±°ë¦¬ í†µê³„
        print(f"\n8. ì´ë™ ê±°ë¦¬ í†µê³„:")
        distance_stats = df.select(
            avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬"),
            min("distance_meter").alias("ìµœì†Œ_ì´ë™ê±°ë¦¬"),
            max("distance_meter").alias("ìµœëŒ€_ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(distance_meter, 0.5)").alias("ì¤‘ì•™ê°’_ì´ë™ê±°ë¦¬")
        )
        
        distance_stats.show()
        
        # ì‹œê°„ëŒ€ë³„ ì´ìš© íŒ¨í„´
        print(f"\n9. ì‹œê°„ëŒ€ë³„ ì´ìš© íŒ¨í„´:")
        hourly_pattern = df.withColumn("hour", hour("rental_date")) \
                           .groupBy("hour") \
                           .count() \
                           .orderBy("hour")
        
        hourly_pattern.show(24)
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        print(f"\n10. ë°ì´í„° í’ˆì§ˆ ê²€ì¦:")
        null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
        print("ì»¬ëŸ¼ë³„ NULL ê°’ ê°œìˆ˜:")
        null_counts.show()
        
        # ì´ìƒì¹˜ íƒì§€
        print(f"\n11. ì´ìƒì¹˜ íƒì§€:")
        outliers = df.filter(
            (col("usage_min") > 480) |  # 8ì‹œê°„ ì´ìƒ
            (col("distance_meter") > 30000) |  # 30km ì´ìƒ
            (col("usage_min") < 0) |  # ìŒìˆ˜ ì‹œê°„
            (col("distance_meter") < 0)  # ìŒìˆ˜ ê±°ë¦¬
        )
        
        outlier_count = outliers.count()
        print(f"ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count:,}ê±´ ({(outlier_count/total_records)*100:.2f}%)")
        
        if outlier_count > 0:
            print("ì´ìƒì¹˜ ìƒ˜í”Œ:")
            outliers.select("rental_id", "usage_min", "distance_meter", "district").show(5)
        
        print(f"\n=== Data Steward Analysis ì™„ë£Œ ===")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {total_records:,}ê±´")
        print(f"ğŸ”‘ ê¶Œí•œ: ì „ì²´ ë°ì´í„° ì ‘ê·¼ (ê°œì¸ì •ë³´ í¬í•¨)")
        print(f"ğŸ“Š ì—­í• : ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë° ê±°ë²„ë„ŒìŠ¤")
        print(f"ğŸ—‚ï¸ ì¹´íƒˆë¡œê·¸: S3 Tables (s3tablesbucket)")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
